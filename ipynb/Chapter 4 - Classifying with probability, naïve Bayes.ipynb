{
 "metadata": {
  "name": "",
  "signature": "sha256:e83f4fe67402b34db51c3f3e7a148bfa4f10f3978c731f8d970a33708a3a6aa5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4.- Classifying with probability theory: na\u00efve Bayes\n",
      "====================================================\n",
      "\n",
      "In summary, we want to know the best guess of a class for a given sample and, in addition, a probability estimate to that best guess.\n",
      "\n",
      "4.1.- Classifying with Bayesian decision theory\n",
      "-----------------------------------------------\n",
      "\n",
      "4.2.- Conditional probability\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Bayes rule used to compute probabilities\n",
      "\n",
      "$$\n",
      "p(A|B) = \\frac{p(B|A) \\cdot p(A)}{p(B)}\n",
      "$$\n",
      "\n",
      "4.3- Classifying with conditional probabilities\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "4.4.- Document classification with Bayes\n",
      "----------------------------------------\n",
      "\n",
      "Words are used as features in a document, which we intend to classify\n",
      "\n",
      "4.5.- Classifying text with Python\n",
      "----------------------------------\n",
      "\n",
      "The purpose is to classify a text as *abusive* (``1``) or not (``0``)\n",
      "\n",
      "###4.5.1.- Prepare: making word vectors from text###\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_data_set():\n",
      "    \n",
      "    sentences = [ 'my dog has flea problems help please',\n",
      "                  'maybe not take him to dog park stupid',\n",
      "                  'my dalmation is so cute I love him',\n",
      "                  'stop posting sutpid worthless garbage',\n",
      "                  'mr licks ate my steak how to stop him',\n",
      "                  'quit buying worthless dog food stupid' ]\n",
      "    \n",
      "    posting_list = [ s.split() for s in sentences ]\n",
      "    posting_class = [0,1,0,1,0,1]   # 1 means abusive, 0 does not\n",
      "    \n",
      "    return (posting_list, posting_class)\n",
      "\n",
      "def create_vocab_list(data_set):\n",
      "    \"\"\"\n",
      "    Creates a list of words (unique) that are present in the document\n",
      "    \n",
      "    >>> create_vocab_list([\"I am a dog not a cat\".split(), \\\n",
      "                           \"I am a jiraffe\".split()])\n",
      "    ['a', 'not', 'I', 'jiraffe', 'am', 'dog', 'cat']\n",
      "    \"\"\"\n",
      "    \n",
      "    vocab_set = set([])\n",
      "    \n",
      "    for document in data_set:\n",
      "        vocab_set |= set(document)\n",
      "        \n",
      "    return list(vocab_set)\n",
      "        \n",
      "def verify_document_words(document, vocabulary_list):\n",
      "    \"\"\"\n",
      "    This method takes the words present in the document and checks if each word\n",
      "    is listed in the vocabulary list, it returns a vector of the same lenght\n",
      "    than vocabulary list in which each position (i.e. word) will have a 1 or 0\n",
      "    depending on wether the word is in document (1) or not (0).\n",
      "    \n",
      "    >>> verify_document_words( \"I am a dog not a cat\".split(),[\"dog\", \"jiraffe\", \"mouse\", \"cat\", \"moose\"])\n",
      "    [1, 0, 0, 1, 0]\n",
      "    \"\"\"\n",
      "    \n",
      "    return [ 1 if w in document else 0 for w in vocabulary_list ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_posts, list_of_classes = create_data_set()\n",
      "vocabulary_list = create_vocab_list( list_of_posts )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['cute',\n",
        " 'love',\n",
        " 'help',\n",
        " 'garbage',\n",
        " 'I',\n",
        " 'problems',\n",
        " 'is',\n",
        " 'park',\n",
        " 'steak',\n",
        " 'stop',\n",
        " 'flea',\n",
        " 'dalmation',\n",
        " 'ate',\n",
        " 'food',\n",
        " 'not',\n",
        " 'him',\n",
        " 'buying',\n",
        " 'posting',\n",
        " 'quit',\n",
        " 'worthless',\n",
        " 'licks',\n",
        " 'how',\n",
        " 'maybe',\n",
        " 'please',\n",
        " 'dog',\n",
        " 'to',\n",
        " 'stupid',\n",
        " 'so',\n",
        " 'take',\n",
        " 'mr',\n",
        " 'sutpid',\n",
        " 'has',\n",
        " 'my']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verify_document_words(list_of_posts[0], vocabulary_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verify_document_words(list_of_posts[1], vocabulary_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 1,\n",
        " 1,\n",
        " 1,\n",
        " 0,\n",
        " 1,\n",
        " 0,\n",
        " 0,\n",
        " 0,\n",
        " 0]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###4.5.2.- Train: Calculating probabilities from word vectors###\n",
      "\n",
      "Using the notation of $\\bf{w}$ for word and $c_i$ for the $i$-th class, we can formulate the statement *probability to be of class $i$-th given the fact that the list/vector/set of words ${\\bf w}$ is present* using Bayes:\n",
      "\n",
      "$$\n",
      "p(c_i|{\\bf w}) = \\frac{p({\\bf w}|c_i) \\cdot p(c_i)}{p({\\bf w})}\n",
      "$$\n",
      "\n",
      "where:\n",
      "- $p(c_{i})$ is the probability to find the $i$-th class in the dataset. In our class, if we want to find the probability for the class *abusive* we note that it appears 3 times in our data set of 6 elements. Therefore $p(abusive) = 3/6 = 0.5$\n",
      "- $p({\\bf w})$ is the probability to find all tokens. This can be rather difficult to compute, however we actually do not care about this value because all class probabilites $p(c_i|{\\bf w})$ are dividied by this same amount. Therefore, in relative terms it is not relevant to include this element.\n",
      "- $p({\\bf w}|c_i)$ is the probability of finding the set of words ${\\bf w}$ given that the document is of $i$-th class. To compute this term we need the assumption that all the words occur *independently* (hence *na\u00efve Bayes*). Under this assumption, we can compute this probability as:\n",
      "\n",
      "$$\n",
      "p({\\bf w}|c_i) = \\prod_{k=1}^N p(w_k | c_i) = p(w_1 | c_i) \\cdot p(w_2 | c_i)\\cdot \\ldots \\cdot p(w_N | c_i)\n",
      "$$\n",
      "\n",
      "\n",
      "\n",
      "To compute the conditional probability $p({\\bf w}|c_i)$ we have to compute, for each word $w_k$, how many times appears in one class and how many class appears in total:\n",
      "\n",
      "$$\n",
      "p(w_k|c_i) = \\frac{count\\,occurrences\\,of\\,w_k\\,in\\,class\\,c_i}{total\\,count\\,occurrences\\,of\\,w_k}\n",
      "$$\n",
      "\n",
      "###4.5.3.- Test: modifying the classifier for real-world conditions###\n",
      "\n",
      "In order to numerical problems of the classifier, two modifcations will have to be done:\n",
      "\n",
      "1. Initilization to values other than 0 to avoid divisions by 0\n",
      "2. Use the logarithm to compute probabilities to avoid underflow (this forces us to use addition instead of multiplications in order to compute the probabilities in later steps). The use of the logarithm is perfectly ok because it is a **monotonically increasing function**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NumPy will ne needed for the vector operations\n",
      "import numpy as np\n",
      "\n",
      "def trainNB0(train_matrix, train_category):\n",
      "    \"\"\"\n",
      "    \n",
      "    :param train_matrix: Will contain a list of lists, in which all list have the same\n",
      "                         size (the size of our vocabulary list) and represent the \n",
      "                         presence (1) or absence (0) of the tokens in each document\n",
      "                         in the vocabulary list.\n",
      "    :param train_caterory: A vector with the same size than the number of documents \n",
      "                           of train_matrix\n",
      "    :return: A dictionary holding, for each category in train_category, a vector of \n",
      "             the same size than the elements of train_matrix, with the conditional\n",
      "             probability of each token\n",
      "    \"\"\"\n",
      "    n_train_docs = len(train_matrix)\n",
      "    n_categories = len(train_category)\n",
      "    \n",
      "    if n_train_docs != n_categories:\n",
      "        raise ValueError(\"The number of documents and the vector holding the \" +\n",
      "                         \"class for each document have to be of identical size\\n\")\n",
      "    \n",
      "    # Number of total words (tokens). All entries of the train matrix have\n",
      "    # the same size, so we only need to use the first list to know the \n",
      "    # total number of words\n",
      "    n_words = len(train_matrix[0])\n",
      "    \n",
      "    # Probability of category, p(c_i). \n",
      "    # This working example assumes a binary set of categories, the category\n",
      "    # can be only 1 or 0. Therefore, the following instruction computes\n",
      "    # the probability of category 1 (p(c_1))\n",
      "    p_category = sum(train_category) / float(n_train_docs)\n",
      "    \n",
      "    # We will build the conditional probability for all tokens.\n",
      "    # We will need numerators and denominators.\n",
      "    # The numerator in this method will be actually\n",
      "    # A vector whose size is the same as the number of words\n",
      "    #\n",
      "    # Note: We will initialize the numerator to 1s instead of 0s\n",
      "    # and the denominator to 2 instead of 0 to avoid divisions\n",
      "    # by 0 or have exactly 0 probability, which might cause numerical \n",
      "    # problems in further steps\n",
      "    pNum = { cat : np.ones(n_words) for cat in train_category}\n",
      "    pDen = { cat : 2 for cat in train_category}\n",
      "    \n",
      "    # For each document\n",
      "    for i in range(n_train_docs):\n",
      "        \n",
      "        category = train_category[i]\n",
      "        \n",
      "        # Vector addition of the occurrences for this class\n",
      "        pNum[category] += train_matrix[i]\n",
      "        \n",
      "        # Add the token occurrence in this category\n",
      "        pDen[category] += sum(train_matrix[i])\n",
      "        \n",
      "    \n",
      "    # Element wise division\n",
      "    #\n",
      "    # We will use the logarithm to avoid underflow problems later on.\n",
      "    # note that this imply that we will have to add probabilities instead\n",
      "    # of\n",
      "    pCond = { cat : np.log(pNum[cat] / pDen[cat]) for cat in train_category}\n",
      "    \n",
      "    return pCond, p_category"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
        " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
        " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
        " ['stop', 'posting', 'sutpid', 'worthless', 'garbage'],\n",
        " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
        " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list_of_classes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[0, 1, 0, 1, 0, 1]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the train matrix from the vocabulary list and each document\n",
      "train_matrix = []\n",
      "for post in list_of_posts:\n",
      "    train_matrix.append(verify_document_words(post, vocabulary_list))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pConditional, pAbusive = trainNB0(train_matrix, list_of_classes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pConditional[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([ 0.        ,  0.        ,  0.        ,  0.05263158,  0.        ,\n",
        "        0.        ,  0.        ,  0.05263158,  0.        ,  0.05263158,\n",
        "        0.        ,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
        "        0.05263158,  0.05263158,  0.05263158,  0.05263158,  0.10526316,\n",
        "        0.        ,  0.        ,  0.05263158,  0.        ,  0.10526316,\n",
        "        0.05263158,  0.10526316,  0.        ,  0.05263158,  0.        ,\n",
        "        0.05263158,  0.        ,  0.        ])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pAbusive\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "0.5"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this point, we will build the full classifyer and test it with some examples. This assumes a binary set of classes (that can take only 1 or 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def classifyNB(set_to_classify, pVec, pClass1):\n",
      "    \n",
      "    p1 = sum(set_to_classify * pVec[1]) + math.log(pClass1)\n",
      "    p0 = sum(set_to_classify * pVec[0]) + math.log(1.0 - pClass1)\n",
      "    \n",
      "    return 1 if p1 > p0 else 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def testingNB():\n",
      "    \"\"\"\n",
      "    Wrapping method to test the classifyer\n",
      "    \"\"\"\n",
      "    \n",
      "    list_of_posts, list_of_classes = create_data_set()\n",
      "    vocabulary_list = create_vocab_list( list_of_posts )\n",
      "    \n",
      "    train_matrix = []\n",
      "    for post in list_of_posts:\n",
      "        train_matrix.append(verify_document_words(post, vocabulary_list))\n",
      "        \n",
      "    pConditional, pAbusive = trainNB0(train_matrix, list_of_classes)\n",
      "    \n",
      "    for test_entry in [ \"love my dalmation\", \"stupid garbage\"]:\n",
      "        entry_voclist = verify_document_words(test_entry.split(), vocabulary_list)\n",
      "        print \"<%s> classified as: %d\"%(test_entry, classifyNB(entry_voclist, pConditional, pAbusive))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testingNB()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<love my dalmation> classified as: 0\n",
        "<stupid garbage> classified as: 1\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###4.5.3.- Test: modifying the classifier for real-world conditions###\n",
      "\n",
      "Where we modify the code to capture the multiple presence of a word (before we were using a set, where any token was allowed to appear only once)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_bag_of_words(document, vocabulary_list):\n",
      "    \"\"\"\n",
      "    This method takes the words present in the document and checks if each word\n",
      "    is listed in the vocabulary list, it returns a vector of the same lenght\n",
      "    than vocabulary list in which each position (i.e. word) will have a n or 0\n",
      "    depending on wether the word is in document n-times (n) or not (0).\n",
      "    \n",
      "    >>> build_bag_of_words( \"I am a dog not a cat , a dog\".split(),\\\n",
      "                            [\"dog\", \"jiraffe\", \"mouse\", \"cat\", \"moose\"])\n",
      "    [2, 0, 0, 1, 0]\n",
      "    \"\"\"\n",
      "    bag = [0] * len(vocabulary_list)\n",
      "    \n",
      "    for word in document:\n",
      "        if word in vocabulary_list: bag[vocabulary_list.index(word)] += 1\n",
      "    return bag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "build_bag_of_words( \"I am a dog not a cat , a dog\".split(),\\\n",
      "                            [\"dog\", \"jiraffe\", \"mouse\", \"cat\", \"moose\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[2, 0, 0, 1, 0]"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4.6.- Example: Classifying spam email with na\u00efve Bayes\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}