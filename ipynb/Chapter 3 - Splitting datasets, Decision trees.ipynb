{
 "metadata": {
  "name": "",
  "signature": "sha256:a97ad744012d1a583e501fdcf74b1cefd1a32ef8146514f45654244727c54dc6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3.- Splitting datasets one feature at a time: Decision trees\n",
      "============================================================\n",
      "\n",
      "\n",
      "3.1.- Tree construction\n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.1.- Information gain (Entropy)###\n",
      "\n",
      "As defined in [Wikipedia](http://en.wikipedia.org/wiki/Entropy_(information_theory), Entropy is **a measure of uncertainty**. Entropy is larger as the source of information is also more random. The less random, the lower the Entropy.\n",
      "\n",
      "Assuming a dicrete random variable $X$ than has the following possible values: $$ {\u00a0x_1, x_2, ..., x_N }$$\n",
      "\n",
      "will have an Entropy ($H$):\n",
      "\n",
      "$$\n",
      "H = - \\sum_{i=1}^{N} p_i \\cdot \\log_2{p_i} \n",
      "$$\n",
      "\n",
      "where $p_i$ is the probability that the value $x_i$ of the random variable $X$ occurs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "# Method to compute the entropy of a dataset\n",
      "def entropy(data_set):\n",
      "    \"\"\"\n",
      "    We are assuming that data_set is a collection of samples,\n",
      "    and each samples has some features and a label in the last field\n",
      "    \n",
      "    >>> entropy( [[1],[0]] )   # Fair coinflip, maximum uncertainty\n",
      "    1.0\n",
      "    \n",
      "    >>> entropy( [[0],[0]] )   # Loaded coinflip, previsible outcome\n",
      "    0.0\n",
      "    \"\"\"\n",
      "    \n",
      "    n_entries = len(data_set)\n",
      "    \n",
      "    # Let's count the labels so that we can then measure its \n",
      "    # probability within the data_set\n",
      "    label_counts = {}\n",
      "    \n",
      "    for sample in data_set:\n",
      "        \n",
      "        current_label = sample[-1]\n",
      "        if current_label not in label_counts:\n",
      "            label_counts[current_label] = 0\n",
      "            \n",
      "        label_counts[current_label] += 1\n",
      "        \n",
      "    # Compute the Entropy (H)\n",
      "    H = 0\n",
      "    \n",
      "    for key, count in label_counts.iteritems():\n",
      "        p = float(count) / n_entries\n",
      "        \n",
      "        H -= p * math.log(p, 2)\n",
      "    \n",
      "    return(H)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_data_set():\n",
      "    data_set = [[1,1,'yes'],[1,1,'yes'], [1,0,'no'], [0,1,'no'], [0,1,'no'] ]\n",
      "    data_set_feature_labels = ['no surfacing', 'flippers']\n",
      "    \n",
      "    return (data_set, data_set_feature_labels)\n",
      "\n",
      "data_set, data_set_feature_labels = create_data_set()\n",
      "\n",
      "entropy(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "0.9709505944546686"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add a little bit of uncertainty in the data set\n",
      "data_set[0][-1] = 'maybe'\n",
      "entropy(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "1.3709505944546687"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gini impurity** is the probability of choosing an item from the set and the probability of that item from being missclassified"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.2.- Splitting the data set###\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split(data_set, column, value):\n",
      "    \n",
      "    split_data_set = []\n",
      "    \n",
      "    for sample in data_set:\n",
      "        if sample[column] == value:\n",
      "            \n",
      "            reduced_sample = sample[:column]\n",
      "            reduced_sample.extend( sample[column+1:] )\n",
      "            \n",
      "            split_data_set.append(reduced_sample)\n",
      "            \n",
      "    return split_data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Have a look at the data set\n",
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all the features whose first column (0) takes the value of 1\n",
      "split(data_set, 0, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all the features whose first column (0) takes the other possible\n",
      "# value (0)\n",
      "split(data_set, 0, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[[1, 'no'], [1, 'no']]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Split based in Information Gain**\n",
      "\n",
      "The choice fo the best feature to split the data set is based on the information gain for the resulting split.\n",
      "\n",
      "Formally, the information gain is defined as\n",
      "\n",
      "> Information gain ($IG(S,A)$) due to a split of the data set \n",
      "> $S$ using feature $A$ is the reduction of entropy (disorder)\n",
      "> that results by splitting using feature $A$\n",
      "\n",
      "The expression is the following:\n",
      "\n",
      "$$\n",
      "IG(S,A) = H(S) - \\sum_{a \\, \\in \\, values(A)} \\frac{|S_a|}{|S|} \\cdot H(S_a)\n",
      "$$\n",
      "\n",
      "where $v$ denotes the value that the feature $A$ can take and $|X|$ denotes the number of elements in the set $X$. The intuition is that we to maximize the information gain when we split, meaning that we want to construct sets with the less amount of randomness possible (i.e. homogeneous split sets). In the event that the split by a feature gives random sets of elements, the part of the summatory will be close to the total entropy and therefore the information gain will be very low. In the event that there is some feature that clearly splits the data set in a group of distinct features (reducing the randomness), the second term of the information gain equation will tend to 0 (low disorder) and therefore the total information gain will be close to the total entropy, thus yielding a large information gain."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division   # To avoid rounding of integer division\n",
      "\n",
      "def choose_best_split(data_set):\n",
      "    \n",
      "    # Making the split will imply that we are going to reduce \n",
      "    # the number of features for the following splits\n",
      "    n_features = len(data_set[0]) - 1 \n",
      "    \n",
      "    # Some initializations\n",
      "    base_entropy = entropy( data_set )\n",
      "    best_info_gain = 0.0\n",
      "    best_feature = None\n",
      "    \n",
      "    # Loop over all the number of features\n",
      "    for i in range(n_features):\n",
      "        \n",
      "        # Build a list of unique values that the feature can take\n",
      "        feature_values = set( [ sample[i] for sample in data_set ] )\n",
      "        \n",
      "        # Calculate the entropy due to this split\n",
      "        new_entropy = 0.0\n",
      "        for value in feature_values:\n",
      "            sub_data_set = split(data_set, i, value)\n",
      "            \n",
      "            p = len(sub_data_set) / len(data_set)\n",
      "            new_entropy += p * entropy(sub_data_set)\n",
      "            \n",
      "        info_gain = base_entropy - new_entropy\n",
      "            \n",
      "        if info_gain > best_info_gain:\n",
      "            best_info_gain = info_gain\n",
      "            best_feature = i\n",
      "            \n",
      "    return best_feature\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "choose_best_split(data_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3.1.3.- Recursively building the tree###\n",
      "\n",
      "\n",
      "This will use recursion to traverse the branches of the tree and recursively split the nodes in new branches.\n",
      "\n",
      "Recursion stops under the following conditions\n",
      "* Run out of attributes to split\n",
      "* All elements of the node are from the same class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We create a method to decide which is the name we will assign the leaf\n",
      "# in the event that we ran out of attributes (i.e. the recursion stopped)\n",
      "# but there are elements of different classes. In this case we will \n",
      "# take a majority vote\n",
      "import operator\n",
      "\n",
      "def majority_count(class_list):\n",
      "    \n",
      "    class_count = {}\n",
      "    \n",
      "    for vote in class_list:\n",
      "        \n",
      "        if vote not in class_count: \n",
      "            class_count[vote] = 0\n",
      "        class_count[vote] += 1\n",
      "        \n",
      "        \n",
      "    sorted_class_count = sorted( class_count.iteritems(),\n",
      "                                 key=opeartor.itemgetter(1),\n",
      "                                 reverse = True )\n",
      "    \n",
      "    return sorted_class_count[0][0]\n",
      "\n",
      "\n",
      "def create_tree(data_set, data_set_feature_labels):\n",
      "    \n",
      "    # Build the list of classes available in the data set\n",
      "    class_list = [ sample[-1] for sample in data_set]\n",
      "\n",
      "    # First condition to end the recursion is that all the\n",
      "    # elements in this set have the same class \n",
      "    if class_list.count(class_list[0]) == len(class_list):\n",
      "        return class_list[0]\n",
      "    \n",
      "    # Second condition to end the recursion, we end the\n",
      "    # features to be used to split the data_set\n",
      "    if len(data_set[0]) == 1:\n",
      "        return majority_count(class_list)\n",
      "    \n",
      "    # If we hit this point, it means that we still have\n",
      "    # room to split the data_set and create new branches.\n",
      "    # Therefore we proceed to choose the following best feature\n",
      "    # to split the tree, the one that offers less disorder (less\n",
      "    # entropy, maximum information gain) in the new branches\n",
      "    best_feature = choose_best_split( data_set )\n",
      "    \n",
      "    best_feature_label = data_set_feature_labels[ best_feature ]\n",
      "    \n",
      "    tree = { best_feature_label : {} }\n",
      "    del( data_set_feature_labels[best_feature] )\n",
      "    \n",
      "    # Build a unique list of values so that we will create a node\n",
      "    # for each of these values (and re-launch the recursion for \n",
      "    # each of these values)\n",
      "    feature_values = set( [ sample[best_feature] for sample in data_set ] )\n",
      "    \n",
      "    for value in feature_values:\n",
      "        \n",
      "        sub_labels = data_set_feature_labels[:]\n",
      "        \n",
      "        split_data_set = split(data_set, best_feature, value)\n",
      "        tree[best_feature_label][value] = create_tree( split_data_set, sub_labels )\n",
      "        \n",
      "    return tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set, data_set_feature_labels = create_data_set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_set_feature_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "['no surfacing', 'flippers']"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = create_tree(data_set, data_set_feature_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3.2.- Plotting trees with matplotlib\n",
      "------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}